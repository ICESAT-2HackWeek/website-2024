{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac80ffba-9f22-4fe4-8c23-28f474e7fe4c",
   "metadata": {},
   "source": [
    "# Putting Open Science into Practice Tutorial\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lesson we will create a dataset in a NetCDF file format that conforms to CF-Conventions and best practices for n-dimensional data using `xarray` and `rioxarray`.  As an example, we will use a subset of the NSIDC Passive Microwave Sea Ice Concentration data ([NSIDC-0051](https://nsidc.org/data/nsidc-0051/versions/2)).  This data is already in NetCDF format and complies with CF-Conventions.  But for the purposes of this tutorial, the data and mask values have been extracted into numpy arrays so that a CF-Compliant dataset can be created from scratch. \n",
    "\n",
    "```{note}\n",
    "Although we are creating a CF-compliant NetCDF file in this tutorial the concepts and best practices demonstrated here can be applied to creating GeoTiff, Shapefile, GeoJSON or csv files.\n",
    "```\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial you will:\n",
    "1.  understand how to make data interoperable and reusable;\n",
    "2.  know why licenses are important for open data and open code;\n",
    "3.  know how to assign a persistent identifier to your data and code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2599c3f4-fa19-4dda-97e8-02e78d45f9d1",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "This week all of us have written some code, processed some data and maybe even produced some results.  The next step is to make sure that this effort is not lost or forgotten by you, your project group, other hackweek particpants, and even the Earth science community.\n",
    "\n",
    "In a weeks time, or maybe longer, will you be able to open a data file or notebook and pick up where you left off?  Will you project team members be able to take your code or data and start working on it?\n",
    "\n",
    "_Take a moment to think about this._\n",
    "\n",
    "What could you do to increase the chance of getting back into the project quickly?\n",
    "\n",
    "_Call out some strategies_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8231545b-0e23-4013-949a-3d833bf1fd12",
   "metadata": {},
   "source": [
    "## Computing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402be761-4774-4f27-a2f1-e9d0f4665714",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATAPATH = Path(\"example_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283e65cc-4470-448a-91fe-8807e4e5cb85",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## The Example Data\n",
    "\n",
    "Let's assume that we have (re)created one month of [Sea Ice Concentrations from Passive Microwave Brightness Temperatures](https://nsidc.org/data/nsidc-0051/versions/2) for December 2022.  As the data producer, you know (at least at the moment) all the details about the dataset.\n",
    "\n",
    "- The data values are sea ice concentrations expressed as a fraction.\n",
    "- Data are created for each day from 2022-12-01 to 2022-12-31.\n",
    "- The Coordinate Reference System for the data is the NSIDC North Polar Stereographic (EPSG:3411)\n",
    "- The grid resolution is 25 km by 25 km\n",
    "- The projected ($x$,$y$) coordinates of the upper-left corner of the upper-left grid cell (the grid origin) are (-3850 km, 5850 km).\n",
    "- The retrievals for December 2022 are from the SSMI instrument onboard the F17 satellite\n",
    "- Some of the grid cells are masked because they are land, too close to the coast and contaminated by returns from the land, or because they fall in the \"Pole Hole\" that is not \"seen\" by the sensor.\n",
    "- The mask values are 251 for the pole hole, 252 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4327977f-8658-423c-8d5c-cee88bacb07e",
   "metadata": {},
   "source": [
    "We'll load the data and see what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e496b5-65b2-4f7d-a512-b564ee71b2c5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(DATAPATH / \"sic.202212.npy\", \"rb\") as f:\n",
    "    data = np.load(f)\n",
    "\n",
    "print(data.shape)\n",
    "plt.imshow(data[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859a4adc-99ba-4f1b-b677-29611319bd9f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(DATAPATH / \"sic.mask.npy\", \"rb\") as f:\n",
    "    mask = np.load(f)\n",
    "\n",
    "print(mask.shape)\n",
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3928f5c2-d765-44fc-ae9a-a615b1f97892",
   "metadata": {},
   "source": [
    "## 1. Create an `xarray.Dataset` for the data\n",
    "\n",
    "`xarray` was designed around the NetCDF file structure and CF-Conventions so it is perfect tool for creating CF-compliant NetCDF files.\n",
    "\n",
    "To create a dataset, we pass a Python dictionary with the variable names as the keys and tuples containing the dimensions of each data variable and the numpy array containing the data.\n",
    "\n",
    "```{note}\n",
    "We can pass a lot more information to create a richer CF-compliant dataset with coordinate variables and variable attributes when we create a `Dataset`.  But for the purposes of this tutorial we will add coordinates and attributes step-by-step.  I'll show an example of defining a Dataset in _one go_ at the end of the tutorial.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ef03a-17c9-494f-aed2-7daf9328373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.Dataset(\n",
    "    {\n",
    "        \"sic\": ((\"time\", \"y\", \"x\"), data), \n",
    "        \"sic_mask\": ((\"y\", \"x\"), mask)\n",
    "    }\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551067c3-84a6-4649-9e05-afc53a1a8df1",
   "metadata": {},
   "source": [
    "Variable, dimension, attribute and group names should start with a letter but can be composed of upper and lower case letters (`A-Za-z`), digits (`0-9`) and underscores (`_`).  It is helpful to keep variable names short and following domain specific conventions.  Avoid just making names up when a commonly used term is available.  In this example, `sic` is a commonly used acronym for Sea Ice Concentration.  The [CF-Standard Name Table](https://cfconventions.org/Data/cf-standard-names/current/build/cf-standard-name-table.html) includes standard variable names from the Atmospheric Model Intercomparison Project (AMIP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a2cf4-778a-489a-9dfd-81b64883d9c0",
   "metadata": {},
   "source": [
    "## 2. Coordinates are the key knowing where your data are located\n",
    "\n",
    "Coordinates locate data in space and time.  In netCDF, coordinates are contained in coordinate variables.  A coordinate variable is defined as a 1-dimensional variable with the same name as its dimension. \n",
    "\n",
    "```\n",
    "float time[time]:\n",
    "```\n",
    "\n",
    "Common coordinates for data are _latitude_, _longitude_, _vertical_, and _time_.  For data in projected coordinate systems spatial coordinates are _x_ and _y_.  See the [Coordinate Types](https://cfconventions.org/Data/cf-conventions/cf-conventions-1.11/cf-conventions.html#coordinate-types) of the CF-Conventions document for more information.\n",
    "\n",
    "For our sea ice concentration dataset we have three coordinates to define: `x`, `y` and `time`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc5ee6f-a91f-4748-aaad-2601413b497f",
   "metadata": {},
   "source": [
    "### Assign spatial (`x`,`y`) coordinates\n",
    "\n",
    "The dataset has the spatial dimensions `x` and `y` but no geospatial coordinates; only mage coordinates that span 0 to 303 on the x-axis and 0 to 447 on the y-axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a3538-fd22-456f-8d2d-003878880b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sic_mask.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530ad485-ecfd-4201-a4f2-a8bb6047e20a",
   "metadata": {},
   "source": [
    "We need to assign geospatial coordinates that map to the image coordinates.  The geospatial coordinates are in a projected coordinate system, the NSIDC North Polar Stereographic.  We \"designed\" the grid with the following attributes.\n",
    "\n",
    "- grid cell width: 25000. m\n",
    "- grid cell height: 25000. m\n",
    "- x-coordinate upper-left corner of upper left grid: -3850000. m\n",
    "- y-coordinate upper-left corner of upper left grid: 5850000 m\n",
    "\n",
    "Note grid parameters have the units of meters.\n",
    "\n",
    "We can use these parameters to calculate the `x` and `y` coordinates of grid cell centers.\n",
    "\n",
    "$$\n",
    "x = (col + 0.5) \\times width + x\\_coord\\_ul \\\\\n",
    "y = (row + 0.5) \\times height + y\\_coord\\_ul\n",
    "$$\n",
    "\n",
    "where $col$ and $row$ are the column and row indices in image coordinates.\n",
    "\n",
    "This can also be expressed using linear algebra.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "width & 0. & x\\_coord\\_ul\\\\\n",
    "0. & height & y\\_coord\\_ul\\\\\n",
    "0. & 0. & 0.\n",
    "\\end{bmatrix}\n",
    "\\begin{pmatrix}\n",
    "col + 0.5\\\\\n",
    "row + 0.5\\\\\n",
    "1.\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "x\\\\\n",
    "y\\\\\n",
    "0.\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This is called an Affine matrix.  We can use the python package `affine` to calculate `x` and `y` coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f60a3e-8759-49e6-8e81-3722caf5c0cc",
   "metadata": {},
   "source": [
    "First, we define the affine matrix and assign this to the dataset using to `rioxarray` `rio` accessor `write_transform`.  The `nsidc_polar_stereo_north` coordinate variable is added to the dataset.  This is the name of the _grid mapping_ that relates coordinates to the Earth.  If you click on the file icon, you can see that `nsidc_polar_stereo_north` has one attribute; GeoTransform.  This contains the same information as the affine matix but is in a slightly different form used by GDAL, which is the backend to `rasterio` and `rioxarray`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2d3ea5-195a-460d-9f76-6ea22fdaace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from affine import Affine\n",
    "transform = Affine(25000., 0., -3850000, 0., -25000., 5850000.)\n",
    "ds.rio.write_transform(transform, inplace=True, grid_mapping_name=\"nsidc_polar_stereo_north\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8030e9e-5b15-42bc-a7d0-26337ae25a72",
   "metadata": {},
   "source": [
    "The affine matrix can be used for matrix multiplication as\n",
    "\n",
    "```\n",
    "(x, y) = affine_matrix * (col, row)\n",
    "```\n",
    "\n",
    "`col` and `row` can be integers or arrays.  So here, I use the `numpy.arange` array generator to create column and row indices of the grid cell centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361d0e0c-92ee-4295-a735-3d25fdfb6477",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.assign_coords(\n",
    "    {\n",
    "        \"x\": (ds.rio.transform() * (np.arange(ds.sizes['x'])+0.5, 0))[0],\n",
    "        \"y\": (ds.rio.transform() * (0, np.arange(ds.sizes['y'])+0.5))[1]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407d3089-3430-41ea-a77e-bef7ea52b6f3",
   "metadata": {},
   "source": [
    "Now, when we plot `mask`, the x and y coordinates are in the projected coordinate system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61778b46-025c-40a6-a8ea-c7a822b7aa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sic_mask.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4381bf64-79ae-4d9c-ba55-95e2f6302e4a",
   "metadata": {},
   "source": [
    "We need to do one final step.  We have a coordinate system that can be used to locate grid cells in projected coordinate space but we do not have any information on how these coordinates relate to the Earth; the _grid mapping_.  This is the Coordinate Reference System (CRS).  This can be done using the `rioxarray` `rio` accessor method `write_crs`, which takes an EPSG code.\n",
    "\n",
    "Clicking on the file icon to see the attributes of `nsidc_polar_stereo_north` we can see that a lot more information has been added.  This information includes the CRS in Well Known Text (WKT) format, along with the projection parameters and ellipsoid definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c88b7d-c690-4060-be8d-050a29994fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.rio.write_crs(3411, inplace=True, grid_mapping_name=\"nsidc_polar_stereo_north\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c436103-25a6-42fd-957e-a6301eda7046",
   "metadata": {},
   "source": [
    "### Assign the `time` coordinate\n",
    "\n",
    "We can locate grid cells in space but not in time.  We need to define a time coordinate.  The most convenient way to do this is to use `pandas.date_range`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c084eb-b170-49e4-bedb-ff22cefdc916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "time = pd.date_range('2022-12-1', '2022-12-31', freq='D')\n",
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f6f2a4-5c19-46f0-998a-1dd4f2cbc7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.assign_coords({'time': time})\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2618a765-d0de-4246-a9db-b291de51211d",
   "metadata": {},
   "source": [
    "## 3. But what do these variables mean?\n",
    "\n",
    "At the moment, the data variables and coordinate variables are just numbers.  We don't know anything about what these numbers represent.  We need to add attributes to describe the variables.\n",
    "\n",
    "All variables are required to have a `units` attribute if the data are dimensional.  If a data variable is `air_temperature` then it should have units Kelvin or degrees Celsius.  For dimensionless data, such as sea ice concentration, a `units` attribute is not required but to avoid ambiguity the `units` attribute can be set to 1.\n",
    "\n",
    "`long_name` and `standard_name` attributes are not required but use of at least one of these is recommended.  `long_name` provides a description of the variable. that can be used for labelling plots  For the `sic` variable, an appropriate `long_name` is `\"sea ice concentration\"`.\n",
    "\n",
    "The purpose of `standard_name` is to provide an unambiguous description of the variable and facilitate interoperability.  `standard_name` attributes should be from a [controlled vocabulary](https://en.wikipedia.org/wiki/Controlled_vocabulary) defined as part of CF-Convention [Standard Name Table](https://cfconventions.org/Data/cf-standard-names/current/build/cf-standard-name-table.html).  For sea ice concentration, the `standard_name` is `sea_ice_area_fraction`.  `standard_names` should be lower case and use `_` instead of spaces.  The expected units are also provided in the [Standard Name Table](https://cfconventions.org/Data/cf-standard-names/current/build/cf-standard-name-table.html).  For more information on standard names see [Section 3.3 of CF Convention 1.11](https://cfconventions.org/Data/cf-conventions/cf-conventions-1.11/cf-conventions.html#standard-name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf11f3d8-c126-4512-b856-b1e25db43d9d",
   "metadata": {},
   "source": [
    "### Add attributes for data variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d442ea6-8495-4266-9046-80be4348efc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"sic\"].attrs = {\n",
    "    \"units\": 1,\n",
    "    \"long_name\": \"Sea Ice Concentration\",\n",
    "    \"standard_name\": \"sea_ice_area_fraction\",\n",
    "    \"grid_mapping_name\": \"nsidc_polar_stereo_north\",\n",
    "    }\n",
    "\n",
    "ds[\"sic_mask\"].attrs = {\n",
    "    \"long_name\": \"sea ice mask flags\",\n",
    "    \"standard_name\": \"sea_ice_area_fraction_mask\",\n",
    "    \"flag_values\": \"251 252 253 254\",\n",
    "    \"flag_meanings\": \"pole_hole_mask unused coast land\",\n",
    "    \"grid_mapping_name\": \"nsidc_polar_stereo_north\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd9b1b6-b4f6-4217-a89f-085a241c711c",
   "metadata": {},
   "source": [
    "### Add attributes for coordinate variables\n",
    "\n",
    "Coordinate variables also need attributes.  If a dataset has geographic spatial coordinates, i.e. _Latitude_ and _Longitude_, only `units`, `long_name` and `standard_name` attributes are necessary.  For example:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"lat\": {\n",
    "        \"long_name\": \"latitude\",\n",
    "        \"units\": \"degrees_north\",\n",
    "        \"standard_name\": \"latitude\",\n",
    "    },\n",
    "    \"lon\": {\n",
    "        \"long_name\": \"longitude\",\n",
    "        \"units\": \"degrees_east\",\n",
    "        \"standard_name\": \"longitude\",\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The Sea Ice Concentration data set is a projected coordinated system; the NSIDC Polar Stereographic North.  The attributes follow the same pattern as for latitude and longitude with `long_name`, `units` and `standard_name`.\n",
    "\n",
    "We'll add these to `attrs` Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3085897e-4ae0-4e79-96bb-32adcd616e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"y\"].attrs = {\n",
    "    \"long_name\": \"y coordinate of projection\",\n",
    "    \"units\": \"m\",\n",
    "    \"standard_name\": \"projection_y_coordinate\",\n",
    "    }\n",
    "\n",
    "ds[\"x\"].attrs = {\n",
    "    \"long_name\": \"x coordinate of projection\",\n",
    "    \"units\": \"m\",\n",
    "    \"standard_name\": \"projection_x_coordinate\",\n",
    "     }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baf1fbd-2dfe-4b5b-ab2d-a29eb7bd98ff",
   "metadata": {},
   "source": [
    "#### Time is precious!\n",
    "\n",
    "The other coordinate that needs attributes is `time`.  Times are stored in NetCDF as increments from a start date or time.  The `units` attribute is required to define the time unit for the increment and the datetime that increments start from.  An example is\n",
    "\n",
    "`days since 1900-01-01 00:00:00`\n",
    "\n",
    "`seconds`, `minutes`, `hours`, and `days` are all acceptable time increments.  Do **not** use _years_ or _months_ because these are not constant in length.  You can also specify a time zone.  The default time zone is UTC.\n",
    "\n",
    "Only `units` are required attributes for a time coordinate.  However, it is useful to add a `long_name` so that labels are automatically generated in plotting functions.   \n",
    "\n",
    "It is also helpful to add a `calendar` attribute.  Most observational datasets will use the [_Gregorian_ calendar](https://en.wikipedia.org/wiki/Gregorian_calendar), which is the calendar used in most parts of the world.  This is the default `standard` calendar, which is a mixed Julian/Gregorian calendar.  Atmospheric reanalyses, such as ERA5, MERRA-2 and CFSR also use the Gregorian Calendar.  However, climate models sometimes use 360 day years and ignore leap years. \n",
    " Dates before 1582-10-15 00:00:00 depend on if the `standard` (mixed Julien/Gegorian) or Proleptic Gregorian (which extends the Gregorian Leap Year rules back before the Gregorian mandate).  \n",
    "\n",
    "See [Section 4.1.1 Calendar](https://cfconventions.org/Data/cf-conventions/cf-conventions-1.11/cf-conventions.html#calendar) for more information.\n",
    "\n",
    "```{warning}\n",
    "**There is no year zero**!  Or is there.  It depends on the calendar.  Some of you may remember the endless debates about whether 1999 or 2000 was the end of the Milennium.  Basically it came down to whether we started counting years at 1 or 0.  And what happens when you switch from Common Era to Before Common Era.  Do dates count back year 1 CE, year 0 CE, year 1 BCE; or year 1 CE, year 1 BCE.  It is best (in my mind at least) to avoid the trouble and assign units avoiding the ambiguity of year zero.  Some tools breaks when there is a year zero.  Either way, there is definitely no month or day zero.  So `days since 0-0-0 00:00:00` is meaningless!\n",
    "```\n",
    "\n",
    "Because the time `units` and `calendar` are used to decode and encode the time coordinate when the file is read and written, `units` and `calendar` are added to the encoding information for the time variable not the attributes.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488a34e0-b2c3-4d2e-9adc-36e268f16b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"time\"].attrs = {\n",
    "    \"long_name\": \"time\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb382b37-5fbb-439d-b35d-0a0c89f32a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051ed58e-6f35-4108-a2e9-8b8b1dcb5fd2",
   "metadata": {},
   "source": [
    "### Add Global Attributes\n",
    "\n",
    "While not required by CF Conventions, it is helpful to provide the title of the dataset, the source, any processing history and a reference for the dataset.  This can increase _reusability_.  Because this information does not apply to any one variable but the dataset as a whole, they are assigned as _global attributes_.\n",
    "\n",
    "Here, we will add `title`, `source`, `processing history` and a `reference`.  `processing_history` could also include any scripts used to create the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5185d6e5-7e6f-4905-8999-b651ae087b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_attrs = {\n",
    "    \"title\": \"NSIDC Passive Microwave Sea Ice Concentration data (NSIDC-0051)\",\n",
    "    \"source\": \"https://doi.org/10.5067/MPYG15WAA4WX\",\n",
    "    \"reference\": (\"DiGirolamo, N., Parkinson, C. L., Cavalieri, D. J., Gloersen, P. \\n\"\n",
    "                 \"& Zwally, H. J. (2022). Sea Ice Concentrations from Nimbus-7 SMMR \\n\"\n",
    "                 \"and DMSP SSM/I-SSMIS Passive Microwave Data, Version 2 [Data Set]. \\n\"\n",
    "                 \"Boulder, Colorado USA. NASA National Snow and Ice Data Center \\n\"\n",
    "                 \"Distributed Active Archive Center. https://doi.org/10.5067/MPYG15WAA4WX. \\n\"\n",
    "                 \"Date Accessed 08-18-2024.\"),\n",
    "    \"processing_history\": (\"Original sea ice concentration grids extracted and non-data\\n\"\n",
    "                           \" values set to _Fill_Value.  Masked grid cells extracted to\\n\"\n",
    "                           \" sic_mask grid\"),\n",
    "}\n",
    "ds.attrs = global_attrs\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84506db8-161a-4e0c-a14e-c33edfe48051",
   "metadata": {},
   "source": [
    "## 4. Preparing the data for writing\n",
    "\n",
    "We now have a dataset and variables that have coordinates that are related to Earth by a Coordinate Reference System (CRS) in the `grid_mapping_name`.  The variables have attributes so that we know what each variable is, how it should be interpretted and what the units are.  We have added some dataset information to help uses understand where the data can from and how it was processing in the global attributes.\n",
    "\n",
    "We now want to write the dataset to a file.  We need to set a couple of _encodings_ that tell the NetCDF library how to write the data to the file.  We need to define a `_FillValue` that is the value used to represent NaN in the file.  We might want to compress the file to save space.  We may also want to chunk the file to help with out of memory operations.  All this information is provided as encodings for each variable.\n",
    "\n",
    "```{note}\n",
    "It is better not to compress coordinate variables.  These are normally 1-dimensional vectors so don't take up as much space as the data variables.  \n",
    "\n",
    "Also, coordinate variables **do not** have missing values so no `_Fill_Value` is set.  If a coordinate value is missing then that coordinate doesn't exist!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f683794c-f55d-4702-9b0d-ab56857fd8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = {\n",
    "    \"sic\": {\n",
    "        \"zlib\": True,   # This uses the zlib library to compress the data\n",
    "        \"complevel\": 9,  # The maximum compression.  For large files this will take a while to write\n",
    "        \"_FillValue\": 9.9692099683868690e+36  # The default fill value for floating point\n",
    "    },\n",
    "    \"sic_mask\": {\n",
    "        \"zlib\": True,   # This uses the zlib library to compress the data\n",
    "        \"complevel\": 9,  # The maximum compression.  For large files this will take a while to write\n",
    "        \"_FillValue\": -127,  # The default fill value for floating point\n",
    "        \"dtype\": \"byte\",\n",
    "    },\n",
    "    \"time\": {\n",
    "        \"units\": \"days since 1970-01-01 00:00:00\",\n",
    "        \"calendar\": \"standard\",  # To be explicit\n",
    "    },\n",
    "}   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d767a51c-76bd-491a-8caa-ae639eb5b1f5",
   "metadata": {},
   "source": [
    "## 5. Write the dataset to a file\n",
    "\n",
    "### NetCDF\n",
    "\n",
    "Now that all the information is in the dataset and we have set up the encoding, the dataset can be written to a file.  This is done using the `to_netcdf` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc32370-90d3-48bb-88a1-5afeacfc9def",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_netcdf(\"test.nc\", encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c821fb73-43e1-4d15-b2aa-9b4ba5ec8c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ncdump -h test.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d88bd12-2c8c-48f1-9a9d-fb045b7a6956",
   "metadata": {},
   "source": [
    "### Zarr\n",
    "\n",
    "We can also create a cloud native Zarr file but we have to change the encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83526b7-7719-49dd-a4c0-2beda086877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_encoding = {\n",
    "    \"sic\": {\n",
    "        \"_FillValue\": 9.9692099683868690e+36  # The default fill value for floating point\n",
    "    },\n",
    "    \"sic_mask\": {\n",
    "        \"_FillValue\": -127,  # The default fill value for floating point\n",
    "        \"dtype\": \"byte\",\n",
    "    },\n",
    "    \"time\": {\n",
    "        \"units\": \"days since 1970-01-01 00:00:00\",\n",
    "        \"calendar\": \"standard\",  # To be explicit\n",
    "    },\n",
    "}   \n",
    "ds.to_zarr(\"test.zarr\", encoding=zarr_encoding, mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2f78d6-b889-46a3-b446-d57e3d093e48",
   "metadata": {},
   "source": [
    "## 6. Read the file you created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a797a16-5bce-4069-a10f-fc10bdb6f1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.open_dataset(\"test.nc\", decode_coords=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eed142-e987-423c-af80-4c109c7d15d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
