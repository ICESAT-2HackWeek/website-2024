{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99216a1c-31e4-4dd0-897c-450337a45a9a",
   "metadata": {},
   "source": [
    "# Making Data Interoperable and Reusable\n",
    "\n",
    "We want to make sure that our data can be used by other researchers as easily as possible.\n",
    "\n",
    "Ask yourself the question, if I dissappear for a month, can someone else open my data files, know what the data is, and start using the data?\n",
    "\n",
    "The [FAIR](https://www.go-fair.org/fair-principles/) principles layout systematic requirements to make data **F**indable, **A**ccessible, **I**nteroperable and **R**eusable.  In this section we will focus on the Interoperable and Reusable parts of FAIR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba08d277-f396-4f37-a363-9afb8045557d",
   "metadata": {},
   "source": [
    "## What is Interoperable data\n",
    "\n",
    "_Interoperability_ means that data can be used by applications and in workflows for analysis, and can be integrated with other data.\n",
    "\n",
    "I tend to think of interoperability as the number of steps (or headaches) required to read, transform and plot data.  A useful guide is to ask the following questions about your dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bba1a3f-b3dc-4975-8679-15c59eab5dd7",
   "metadata": {},
   "source": [
    "(standard-file-format)=\n",
    "### Is the data in a standard file format?\n",
    "\n",
    "Fortunately, the Earth science community has, _mostly_, converged on a standard set of well defined file formats.  {ref}`formats` shows traditional and cloud-optimized formats for some common types of geospatial data.  See the [cloud optimized data](./tutorials/cloud-computing/03-cloud-optimized-data-access.ipynb) tutorial for more information of cloud-optimized file formats.  This tutorial will focus on traditional formats but a lot of what is covered also applies to cloud-optimized formats.\n",
    "\n",
    "```{note}\n",
    "Geospatial data is any data that can be referenced to a location on Earth; either at the surface or above or below the surface.\n",
    "```\n",
    "\n",
    "The choice of which file format to store your data in can be guided the type of data you need to store.\n",
    "\n",
    "```{figure} https://guide.cloudnativegeo.org/images/cogeo-formats-table.png\n",
    "---\n",
    "alt: Table of traditional and cloud-optimized geospatial file formats\n",
    "align: center\n",
    "name: formats\n",
    "---\n",
    "Common traditional and cloud-optimized geospatial file formats for vector, point-cloud and N-dimensional data.  \n",
    "Source: [Cloud-optimized geospatial formats guide](https://guide.cloudnativegeo.org/)\n",
    "```\n",
    "\n",
    "_Vector data_ is data represented as points, lines and polygons.   Each vetor entity can have attributes associated with it.  For example, a network of weather stations would be represented as a set of points.  A ship track or buoy trajectory would be represented as a line.  A set of glacier outlines are a set of polygons.  Vector data are commonly stored as Shapefiles or GeoJSON.\n",
    "\n",
    "```{figure} https://xarray.dev/xarray-datastructure.png\n",
    "---\n",
    "alt: A N-Dimensional data structure for temperature and precipitation\n",
    "align: center\n",
    "name: n-dimensional\n",
    "---\n",
    "A cartoon of a NetCDF-style N-dimensional data structure containing ($time$,$x$,$y$) data-cubes of air temperature and precipitation, and latitude and longitude coordinate variables.  The named dimensions in the file are shown as orthogonal axes to the right of the variables.\n",
    "Source: [xarray-dev](https://xarray.dev/)\n",
    "```\n",
    "\n",
    "_N-dimensional data_ is data with two or more dimensions.  Two examples are a geolocated remote sensing image, and a data-cube ($t$,$x$,$y$) of temperature grids from a climate model (e.g. {ref}`n-dimensional`).  NetCDF4 is commonly used to store N-dimensional datasets because it allows coordinates to be stored with the data variables so that data can be located in space and time, as well as allowing attributes for each data variable to be stored.  _Global_ attributes can also be stored with each file.  This makes the format _self-describing_, which adds to _interoperability_ and _reusability_.\n",
    "\n",
    "The standard format for geolocated image data is GeoTIFF (Geolocated Tagged Image File Format).  GeoTIFF stores information that allows each image pixel to be located on Earth.  Multiple bands can be stored.  However, GeoTIFF files cannot store the rich metadata that NetCDF files can store.\n",
    "\n",
    "_Point cloud data_ are three-dimensional data ($x$,$y$,$z$) with attributes usually produced by Lidar or photogrammetry.  This could be a mapping of a snow surface from Terrestrial Scanning Lidar.  Point cloud data shares characteristics with _Vector data_ but generall has a much higher density of points.\n",
    "\n",
    "A fourth type of data is _Tabular data_, which is data structured as _rows_ and _columns_.  Weather station records are tabular data.  The data below contains air temperature and dewpoint temperature data recorded at 5-minute intervals for the Taos, NM, Soil and Water Conservation District site.   \n",
    "\n",
    "```\n",
    "datetime, air_temperature, dewpoint\n",
    "2024-08-12 16:05:00, 87.2, 55.5\n",
    "2024-08-12 16:10:00, 86.7, 55.6\n",
    "2024-08-12 16:15:00, 86.4, 55.5\n",
    "2024-08-12 16:20:00, 87.3, 55.2\n",
    "2024-08-12 16:25:00, 86.8, 54.8\n",
    "2024-08-12 16:30:00, 87.6, 55.1\n",
    "```\n",
    "\n",
    "Often this data can be stored as a Comma Seperated File (csv).  For small volumes of data csv is fine.  Data are both machine and human readable, which is great for checking data _by hand_.  NetCDF or even HDF5 could be used and would allow attributes to be included.  Cloud optimized formats such as Parquet could also be used.\n",
    "\n",
    "Hierachical Data Format version 5 (HDF5) is widely used by NASA, and other to store complex data structures.  ICESat-2 data is one example.  The Geolocated Photon Height product (ATL03) contains over 1000 variables.  These variables are organized into groups with different dimension sizes.  Each variable has a set of attributes associated with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf934c69-2789-408d-a952-61432bf0525c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(common-tools)=\n",
    "### Can the data be read by common tools?\n",
    "\n",
    "Putting data into the common standard file formats used in your discipline will make it more likely that users of your data can read the data with commonly used tools.  I will focus on Python tools here but R, Julia, and Rust languages, and Matlab and IDL all have ways to read HDF5, NetCDF, GeoTIFF, Shapefiles, GeoJSON, and csv.\n",
    "\n",
    "The common Python tools used for working with geospatial and tabular data are listed in the table below.\n",
    "\n",
    "```{list-table} Common Python tools for read, writing and working with geospatial and tabular data \n",
    ":header-rows: 1\n",
    ":name: tools-table\n",
    "\n",
    "* - Datatype\n",
    "  - Formats\n",
    "  - Tools\n",
    "* - Vector\n",
    "  - Shapefile\n",
    "  - `geoPandas`  \n",
    "    `shapely`\n",
    "* - \n",
    "  - GeoJSON\n",
    "  - `geopandas`  \n",
    "    `shapely`\n",
    "* - N-dimensional data\n",
    "  - NetCDF\n",
    "  - `xarray`  \n",
    "    `rioxarray`\n",
    "* - \n",
    "  - GeoTIFF\n",
    "  - `rioxarray`  \n",
    "    `rasterio`\n",
    "* -\n",
    "  - HDF5\n",
    "   - `xarray`  \n",
    "     `h5py`\n",
    "* - Point cloud\n",
    "  - las\n",
    "  - ??\n",
    "* - Tabular Data\n",
    "  - csv\n",
    "  - `pandas`\n",
    "```\n",
    "\n",
    "Most, if not all, of these tools have write methods to create files in the described in [standard file formats](standard-file-format) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfd5d63-2033-4f56-96e7-e77b19923069",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Does the data follow domain standards and conventions\n",
    "\n",
    "Storing data in standardized data structures and file formats that data can be read and manipulated using a standard set of tools.  By following domain specific conventions and \"best practices\", software packages and machines, and users, can understand (interpret) data.\n",
    "\n",
    "Climate Forecast (CF) Conventions are relevant for most N-dimensional data.  The purpose of CF conventions is to make NetCDF files _self describing_.  \n",
    "\n",
    "{attribution=\"[NetCDF Climate and Forecast (CF) Metadata Conventions](https://cfconventions.org/Data/cf-conventions/cf-conventions-1.11/cf-conventions.html#_introduction), Version 1.11\"}\n",
    "> The purpose of the CF conventions is to require conforming datasets to contain sufficient metadata that they are self-describing in the sense that each variable in the file has an associated description of what it represents, including physical units if appropriate, and that each value can be located in space (relative to earth-based coordinates) and time.\n",
    ">\n",
    "> An important benefit of a convention is that it enables software tools to display data and perform operations on specified subsets of the data with minimal user intervention.\n",
    "\n",
    "The dump of a subsetted ERA5 Reanalaysis file shows the structure of a CF-compliant file\n",
    "\n",
    "```\n",
    "netcdf era5.single_levels.monthly.1959 {\n",
    "dimensions:\n",
    "\tlongitude = 1440 ;\n",
    "\tlatitude = 721 ;\n",
    "\ttime = 12 ;\n",
    "variables:\n",
    "\tfloat longitude(longitude) ;\n",
    "\t\tlongitude:units = \"degrees_east\" ;\n",
    "\t\tlongitude:long_name = \"longitude\" ;\n",
    "\tfloat latitude(latitude) ;\n",
    "\t\tlatitude:units = \"degrees_north\" ;\n",
    "\t\tlatitude:long_name = \"latitude\" ;\n",
    "\tint time(time) ;\n",
    "\t\ttime:units = \"hours since 1900-01-01 00:00:00.0\" ;\n",
    "\t\ttime:long_name = \"time\" ;\n",
    "\t\ttime:calendar = \"gregorian\" ;\n",
    "\tshort t2m(time, latitude, longitude) ;\n",
    "\t\tt2m:scale_factor = 0.00166003112108415 ;\n",
    "\t\tt2m:add_offset = 258.629197755436 ;\n",
    "\t\tt2m:_FillValue = -32767s ;\n",
    "\t\tt2m:missing_value = -32767s ;\n",
    "\t\tt2m:units = \"K\" ;\n",
    "\t\tt2m:long_name = \"2 metre temperature\" ;\n",
    "\tshort tcwv(time, latitude, longitude) ;\n",
    "\t\ttcwv:scale_factor = 0.00104011870543087 ;\n",
    "\t\ttcwv:add_offset = 34.1879291446153 ;\n",
    "\t\ttcwv:_FillValue = -32767s ;\n",
    "\t\ttcwv:missing_value = -32767s ;\n",
    "\t\ttcwv:units = \"kg m**-2\" ;\n",
    "\t\ttcwv:long_name = \"Total column vertically-integrated water vapour\" ;\n",
    "\t\ttcwv:standard_name = \"lwe_thickness_of_atmosphere_mass_content_of_water_vapor\" ;\n",
    "\n",
    "// global attributes:\n",
    "\t\t:Conventions = \"CF-1.6\" ;\n",
    "\t\t:history = \"2022-06-17 18:39:10 GMT by grib_to_netcdf-2.24.3: /opt/ecmwf/mars-client/bin/grib_to_netcdf -S param -o /cache/data7/adaptor.mars.internal-1655491150.0401924-522-6-56f5a114-0c72-40e2-bb43-9866e22ab084.nc /cache/tmp/56f5a114-0c72-40e2-bb43-9866e22ab084-adaptor.mars.internal-1655491144.161893-522-11-tmp.grib\" ;\n",
    "}\n",
    "```\n",
    "\n",
    "NetCDF has the concept of two types of variables: _data variables_ and _coordinate variables_.  In this example, `longitude`, `latitude` and `time` are all coordinate variables.  The coordinate variables are 1-dimensional and have the same names as the three dimensions of the file.  The data variables are `t2m` and `tcwv`.  These are linked to the dimensions and coordinate variables.\n",
    "\n",
    "```{note}\n",
    "Having 1-dimensional coordinate variables is really helpful because you can subset data by coordinate values easily.  Add an example for this\n",
    "```\n",
    "\n",
    "Both coordinate variables and data variables have attributes that describe what each variable is and what the units are.  These are critical for both humans and software to understand what the data are.  There are also _global attributes_ that are used to describe the conventions used, here CF Conventions, version 1.6, and processing history.\n",
    "\n",
    "CF Conventions require spatial coordinate variables (`longitude` and `latitude` in this case) to have `units` and a `long_name` attribute.  `degrees_north` and `degrees_east` are the recommended units for for latitude and longitude.  If data are in projected coordinates (e.g. `x` and `y`), these woud have units `m` for meters.\n",
    "\n",
    "The `time` variable also has `units`, a `long_name` and a `calendar` attribute.  The units have the format _interval since reference_time_ where interval can be `seconds`, `minutes`, `hours` and `days`. \n",
    " `years` and `months` are allowed by not recommended because the length or a year and a month can vary.\n",
    "```\n",
    "days since 1971-04-21 06:30:00.0\n",
    "```\n",
    "\n",
    "A calendar is important because it defines how time is incremented.  Some climate models use 360-day or 365-day calendars with no leap year.  Paleo-data may used a mixed Julian/Gregorian or _proleptic Gregorian_ calendar, in which dates before the switch from Julian to Gregorian calendars follow the Gregorian leap-year rules.\n",
    "\n",
    "Try to avoid a year zero and remember that months and days start at one not zero!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d3375d-b69c-44ed-83d4-33de8a4b8c01",
   "metadata": {},
   "source": [
    "### Does the file contain useful coordinates (time, x, y, z)?\n",
    "\n",
    "Geospatial data needs to be located on Earth and to a date.  So data needs coordinates.  Spatial coordinates may be `latitude`, `longitude` and `height` (or `depth`), or `x`, `y`, and `z` usually in meters.  But heights may be pressure levels, sigma levels, density, potential temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9491bd-4445-4b7d-bdda-9f69c0066062",
   "metadata": {},
   "source": [
    "### Does the file have a coordinate reference system?\n",
    "\n",
    "Coordinate values are associated with a set of axes or coordinate system.  A Coordinate Reference System (CRS) is required to relate the coordinate system to Earth.  Even if your data is in `latitude` and `longitude` it is better to be explicit and give the CRS.  It is not always wise to assume that `latitudes` and `longitudes` are in WGS84 (EPSG:4326)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aaa3c7-8dae-4d0a-b51c-405a77f89629",
   "metadata": {},
   "source": [
    "### Do the variables have standard names, units, etc?\n",
    "\n",
    "Units are just as important now as they were for your physics homework.  Standard names define your data in a standard way so that users know exactly what the variable is. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1466c4c-baed-45df-a81b-31c75cf2e895",
   "metadata": {},
   "source": [
    "## What is **Reusable** data\n",
    "\n",
    "_Reusability_ means that the data have sufficient information about what it is, how it was collected and processed so that future users can understand the data and use it correctly.\n",
    "\n",
    "I think of this as having most of my questions answered about the data.\n",
    "\n",
    "- Is it appropriate to use this data for what I want to do?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dbcc3c-4478-4070-9680-23bfadd76b22",
   "metadata": {},
   "source": [
    "### Is the data structured for analysis?\n",
    "\n",
    "In most cases, vector data are structured so that they can be used in analysis workflows.  However, n-dimensional data and tabular data are often _messy_ and need to be tidied before you can do analysis.  Below is a list of some ways to make your data tidy.\n",
    "\n",
    "```{note}\n",
    "Tidy data is a term from the R community, popularized by Hadley Wickham, for tabular data that is structured for easy analysis.  Here, I also apply it to n-dimensional data.\n",
    "```\n",
    "\n",
    "```{warning}\n",
    "While many of these \"best practices\" are documented by other data managers and users elsewhere, some are my own opinionated \"best practice\".  I make no apologies for this.  Where possible, I give what I hope is a good reason for the practice.  Where I give no reason, it is probably because at some point I got annoyed with having to spend hours cleaning data and the \"best practice\" could have saved me time.\n",
    "```\n",
    "\n",
    "#### \"Tidy\" n-dimensional data\n",
    "\n",
    "- Data variables should contain **only** data values\n",
    "- Try to avoid using groups if at all possible - `xarray` cannot read multiple groups but see `Datatree`\n",
    "\n",
    "#### Tidy tabular data\n",
    "\n",
    "- Above all remember that your data needs to be read by a machine, and machines are dumb!  No matter what ChatGPT or Elon Musk tells you.  You may be able to read a nicely formatted page in Excel but a computer will have a hard time, if not find it impossible.\n",
    "- rows are records - e.g. measurements for a single location, timestep or class - show example\n",
    "- columns are a single variable\n",
    "- Prefer iso-standard datetimes rather than breaking date components into separate columns.  Users (including a future you) will only have to reconstruct the datetimes from these columns.  And tools like `pandas` can select by year, month and day automatically.\n",
    "- Do not include marginal sums and statistics in the data\n",
    "- Avoid using spaces in column names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
