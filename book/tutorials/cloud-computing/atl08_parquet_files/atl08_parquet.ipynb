{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "017000a7-859d-4aad-a89f-754a3973c71b",
   "metadata": {},
   "source": [
    "# Generating a ATL08 GeoParquet Store\n",
    "\n",
    "This notebook creates a [GeoParquet](https://geoparquet.org/) store from scratch using a subset of [ICESat-2 ATL08](https://nsidc.org/data/atl08/versions/6) files. GeoParquet is built on [Apache Parquet](https://parquet.apache.org/) which is an open-source column-oriented file format which allows for efficient storage and retrieval using high performance compression.\n",
    "\n",
    "The conversion functions are in the helpers file atl08_parquet_helpers which are functions originally written by Sean Harkins of Development Seed in https://github.com/developmentseed/icesat-parquet/.\n",
    "\n",
    ":::{warning}\n",
    "This work is experimental\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b32c78-7a35-4ad6-892d-aa459aff2550",
   "metadata": {},
   "source": [
    "## 1. Install and import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddb38b2-7f9f-4be7-8ae4-cb79f01d8607",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarrow geoarrow-pyarrow geopandas earthaccess==0.9.0 jupyterlab_vim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4527f0f3-6233-4bee-98b6-ff9c89ec0d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import atl08_parquet_helpers as aph\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import earthaccess\n",
    "import fsspec\n",
    "import geopandas as gpd\n",
    "from lonboard import viz\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "from shapely import wkb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0e098c-f3c3-4a7b-83b3-d59148310ae3",
   "metadata": {},
   "source": [
    "## 2. Login to earthaccess using [URS credentials](https://urs.earthdata.nasa.gov/home) and then setup an S3 client with credentials for NSIDC DAAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21f6342-6864-41ef-820c-f06e5116a8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "earthaccess.login()\n",
    "\n",
    "aws_creds = earthaccess.get_s3_credentials(daac='NSIDC')\n",
    "\n",
    "s3 = fsspec.filesystem(\n",
    "    's3',\n",
    "    anon=False,\n",
    "    key=aws_creds['accessKeyId'],\n",
    "    secret=aws_creds['secretAccessKey'],\n",
    "    token=aws_creds['sessionToken'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39224451-e684-4bad-9b1d-3dc9eafe01f4",
   "metadata": {},
   "source": [
    "## 3. Search for a subset of ATL08 granules using the [earthaccess](https://github.com/nsidc/earthaccess) library\n",
    "\n",
    "This search is only for 1 week for results over South America."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55b6316-eb17-4ab0-acfb-f3563e5bc316",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(2021, 11, 1, tzinfo=timezone.utc)\n",
    "end = start + timedelta(days=7)\n",
    "\n",
    "results = earthaccess.search_data(\n",
    "    short_name=\"ATL08\",\n",
    "    cloud_hosted=True,\n",
    "    temporal=(start, end),\n",
    "    bounding_box=(-90,-56,-32,14),\n",
    "    count=-1\n",
    ")\n",
    "year_month = f\"year={start.year}/month={start.month}\"\n",
    "week = 0\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f82d0e1-c0d0-4165-ba97-63c8a890ef94",
   "metadata": {},
   "source": [
    "## 4. Sort the results and setup the parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1e3632-7af4-4c3e-aa68-4dd11fc8d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_results = sorted(results, key=lambda r : datetime.strptime(r['umm']['TemporalExtent']['RangeDateTime']['BeginningDateTime'], '%Y-%m-%dT%H:%M:%S.%fZ'))\n",
    "\n",
    "template_file = s3.open(sorted_results[0].data_links(access=\"direct\")[0], 'rb')\n",
    "\n",
    "atl08_parquet = aph.ParquetTable(\n",
    "    geometadata_file='atl08-parquet-metadata.json',\n",
    "    template_file=template_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f17bed-fb90-4386-9b47-05ea56b920a6",
   "metadata": {},
   "source": [
    "## 5. Write results to the parquet table\n",
    "\n",
    "Write results to 1 parquet file, using the year-month as a partition. Later on if we add more weeks we can add them to new parquet files and new partititions as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bc69f4-69d5-4682-be3d-02fdfeebea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# fetch a group and write them to a partition\n",
    "directory=\"atl08_parquet\"\n",
    "os.makedirs(f\"{directory}/{year_month}\", exist_ok=True)\n",
    "# i think it can only go one beam at a time even with more workers because of the global hdf5 interpreter lock\n",
    "atl08_parquet.write_results_by_partition(sorted_results, s3, parquet_file=f\"{directory}/{year_month}/{week}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c41ee4f-5885-475b-930d-ad523f03bb7c",
   "metadata": {},
   "source": [
    "## We're done creating the parquet!\n",
    "\n",
    "Now we can checkout the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852324eb-e248-475f-a764-149c324891f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pq.ParquetDataset(\"atl08_parquet\", partitioning=\"hive\", filters=[('year', '>=', 2021),\n",
    "                                                     ('year', '<=', 2021),\n",
    "                                                     ('month', '>=', 11),\n",
    "                                                     ('month', '<=', 11)])\n",
    "table = dataset.read(columns=[\"h_canopy\", \"geometry\"])\n",
    "df = table.to_pandas()\n",
    "df['geometry'] = df['geometry'].apply(wkb.loads)\n",
    "\n",
    "\n",
    "gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8a1c6f-5317-46dd-aac6-cdcafb1a2ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_value = gdf['h_canopy'].max() \n",
    "gdf_filtered = gdf.loc[gdf['h_canopy'] != null_value]\n",
    "gdf_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f0499b-5cf2-4b68-81b4-3d81a0eee73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_filtered['h_canopy'].min(), gdf_filtered['h_canopy'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebbd2e2-a5da-4473-a82d-3a67e52f8563",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# this will take too long and / or cause the kernel to die with large dataframes\n",
    "# depending on your available memory\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "crs = ccrs.PlateCarree()\n",
    "fig, ax = plt.subplots(subplot_kw=dict(projection=crs))\n",
    "gdf_filtered.plot(column='h_canopy', ax=ax, legend=True, cmap='viridis')\n",
    "ax.set_extent([-116,-23,-32,21])\n",
    "ax.set_title('h_canopy plot')\n",
    "\n",
    "# Add coastlines and gridlines\n",
    "ax.coastlines()\n",
    "ax.gridlines(draw_labels=True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8472a90-04de-44cc-a7ec-1152a68d3e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from lonboard import Map, ScatterplotLayer\n",
    "from lonboard.colormap import apply_continuous_cmap\n",
    "from palettable.colorbrewer.diverging import BrBG_10\n",
    "\n",
    "layer = ScatterplotLayer.from_geopandas(gdf_filtered)\n",
    "h_canopy = gdf_filtered['h_canopy']\n",
    "layer.get_fill_color = apply_continuous_cmap(h_canopy, BrBG_10, alpha=0.7)\n",
    "\n",
    "m = Map(layer)\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
